{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# This line is only needed if you have a HiDPI display\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from sklearn.datasets import make_blobs, make_circles, make_moons\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SMOModel:\n",
    "    \"\"\"Container object for the model used for sequential minimal optimization.\"\"\"\n",
    "    \n",
    "    def __init__(self, X, y, C, kernel, alphas, b, errors):\n",
    "        self.X = X               # training data vector\n",
    "        self.y = y               # class label vector\n",
    "        self.C = C               # regularization parameter\n",
    "        self.kernel = kernel     # kernel function\n",
    "        self.alphas = alphas     # lagrange multiplier vector\n",
    "        self.b = b               # scalar bias term\n",
    "        self.errors = errors     # error cache\n",
    "        self._obj = []           # record of objective function value\n",
    "        self.m = len(self.X)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def linear_kernel(x, y, b=1):\n",
    "    \"\"\"Returns the linear combination of arrays `x` and `y` with\n",
    "    the optional bias term `b` (set to 1 by default).\"\"\"\n",
    "    print \"linear kernel \"\n",
    "    print x.shape\n",
    "    print (y.T).shape\n",
    "    return np.dot(x, y.T) + b # Note the @ operator for matrix multiplication\n",
    "\n",
    "\n",
    "def gaussian_kernel(x, y, sigma=1):\n",
    "    \"\"\"Returns the gaussian similarity of arrays `x` and `y` with\n",
    "    kernel width parameter `sigma` (set to 1 by default).\"\"\"\n",
    "    print \"gaussian kernel \"\n",
    "    print x.shape\n",
    "    print (y.T).shape\n",
    "    if np.ndim(x) == 1 and np.ndim(y) == 1:\n",
    "        result = np.exp(- np.linalg.norm(x - y) / (2 * sigma ** 2))\n",
    "    elif (np.ndim(x) > 1 and np.ndim(y) == 1) or (np.ndim(x) == 1 and np.ndim(y) > 1):\n",
    "        result = np.exp(- np.linalg.norm(x - y, axis=1) / (2 * sigma ** 2))\n",
    "    elif np.ndim(x) > 1 and np.ndim(y) > 1:\n",
    "        result = np.exp(- np.linalg.norm(x[:, np.newaxis] - y[np.newaxis, :], axis=2) / (2 * sigma ** 2))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_len, y_len = 5, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear kernel \n",
      "(5L, 1L)\n",
      "(1L, 10L)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.35447541,  1.24015377,  1.48941707,  1.52887114,  1.21043788,\n",
       "         1.43450939,  1.29026467,  1.31175053,  1.50797993,  1.03898555],\n",
       "       [ 1.4619366 ,  1.31295772,  1.63778657,  1.68920136,  1.2742333 ,\n",
       "         1.56623333,  1.37826002,  1.40625943,  1.66197687,  1.05080423],\n",
       "       [ 1.38932132,  1.26376154,  1.5375281 ,  1.58086062,  1.2311245 ,\n",
       "         1.47722286,  1.31879849,  1.34239646,  1.55791575,  1.04281793],\n",
       "       [ 1.35193684,  1.23843391,  1.48591211,  1.52508363,  1.20893084,\n",
       "         1.43139766,  1.28818595,  1.30951793,  1.50434204,  1.03870635],\n",
       "       [ 1.27363614,  1.18538592,  1.37780391,  1.40826035,  1.16244684,\n",
       "         1.33541811,  1.22406887,  1.2406548 ,  1.39213346,  1.03009477]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_kernel(np.random.rand(x_len, 1), np.random.rand(y_len, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gaussian kernel \n",
      "(5L, 1L)\n",
      "(1L, 10L)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.64034735,  0.70046237,  0.82929857,  0.70701745,  0.98454806,\n",
       "         0.75851261,  0.97227947,  0.65130984,  0.80464064,  0.84894038],\n",
       "       [ 0.61927863,  0.67741575,  0.80201298,  0.68375515,  0.95215446,\n",
       "         0.73355602,  0.94028953,  0.62988044,  0.77816635,  0.82100854],\n",
       "       [ 0.92960153,  0.98340854,  0.83063049,  0.97429092,  0.69965165,\n",
       "         0.90814664,  0.70848012,  0.94551594,  0.85608487,  0.81141231],\n",
       "       [ 0.90462861,  0.98955404,  0.85356064,  0.99881449,  0.71896603,\n",
       "         0.93321668,  0.72803822,  0.92011549,  0.8797177 ,  0.83381193],\n",
       "       [ 0.94714499,  0.96519339,  0.81524517,  0.95624465,  0.68669238,\n",
       "         0.89132553,  0.69535732,  0.96335973,  0.84022807,  0.79638296]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_kernel(np.random.rand(x_len, 1), np.random.rand(y_len, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Objective function to optimize\n",
    "\n",
    "def objective_function(alphas, target, kernel, X_train):\n",
    "    \"\"\"Returns the SVM objective function based in the input model defined by:\n",
    "    `alphas`: vector of Lagrange multipliers\n",
    "    `target`: vector of class labels (-1 or 1) for training data\n",
    "    `kernel`: kernel function\n",
    "    `X_train`: training data for model.\"\"\"\n",
    "    \n",
    "    return np.sum(alphas) - 0.5 * np.sum(target * target * kernel(X_train, X_train) * alphas * alphas)\n",
    "\n",
    "\n",
    "# Decision function\n",
    "\n",
    "def decision_function(alphas, target, kernel, X_train, x_test, b):\n",
    "    \"\"\"Applies the SVM decision function to the input feature vectors in `x_test`.\"\"\"\n",
    "    print \"decision function\"\n",
    "    result = (target * alphas) * kernel(X_train, x_test) - b\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, ax, resolution=100, colors=('b', 'k', 'r')):\n",
    "        \"\"\"Plots the model's decision boundary on the input axes object.\n",
    "        Range of decision boundary grid is determined by the training data.\n",
    "        Returns decision boundary grid and axes object (`grid`, `ax`).\"\"\"\n",
    "        \n",
    "        # Generate coordinate grid of shape [resolution x resolution]\n",
    "        # and evaluate the model over the entire space\n",
    "        xrange = np.linspace(model.X[:,0].min(), model.X[:,0].max(), resolution)\n",
    "        yrange = np.linspace(model.X[:,1].min(), model.X[:,1].max(), resolution)\n",
    "        grid = [[decision_function(model.alphas, model.y,\n",
    "                                   model.kernel, model.X,\n",
    "                                   np.array([xr, yr]), model.b) for yr in yrange] for xr in xrange]\n",
    "        grid = np.array(grid).reshape(len(xrange), len(yrange))\n",
    "        \n",
    "        # Plot decision contours using grid and\n",
    "        # make a scatter plot of training data\n",
    "        ax.contour(xrange, yrange, grid, (-1, 0, 1), linewidths=(1, 1, 1),\n",
    "                   linestyles=('--', '-', '--'), colors=colors)\n",
    "        ax.scatter(model.X[:,0], model.X[:,1],\n",
    "                   c=model.y, cmap=plt.cm.viridis, lw=0, alpha=0.5)\n",
    "        \n",
    "        # Plot support vectors (non-zero alphas)\n",
    "        # as circled points (linewidth > 0)\n",
    "        mask = model.alphas != 0.0\n",
    "        ax.scatter(model.X[:,0][mask], model.X[:,1][mask],\n",
    "                   c=model.y[mask], cmap=plt.cm.viridis)\n",
    "        \n",
    "        return grid, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def take_step(i1, i2, model):\n",
    "    \n",
    "    # Skip if chosen alphas are the same\n",
    "    if i1 == i2:\n",
    "        return 0, model\n",
    "    \n",
    "    alph1 = model.alphas[i1]\n",
    "    alph2 = model.alphas[i2]\n",
    "    y1 = model.y[i1]\n",
    "    y2 = model.y[i2]\n",
    "    E1 = model.errors[i1]\n",
    "    E2 = model.errors[i2]\n",
    "    s = y1 * y2\n",
    "    \n",
    "    # Compute L & H, the bounds on new possible alpha values\n",
    "    if (y1 != y2):\n",
    "        L = max(0, alph2 - alph1)\n",
    "        H = min(model.C, model.C + alph2 - alph1)\n",
    "    elif (y1 == y2):\n",
    "        L = max(0, alph1 + alph2 - model.C)\n",
    "        H = min(model.C, alph1 + alph2)\n",
    "    if (L == H):\n",
    "        return 0, model\n",
    "\n",
    "    # Compute kernel & 2nd derivative eta\n",
    "    k11 = model.kernel(model.X[i1], model.X[i1])\n",
    "    k12 = model.kernel(model.X[i1], model.X[i2])\n",
    "    k22 = model.kernel(model.X[i2], model.X[i2])\n",
    "    eta = 2 * k12 - k11 - k22\n",
    "    \n",
    "    # Compute new alpha 2 (a2) if eta is negative\n",
    "    if (eta < 0):\n",
    "        a2 = alph2 - y2 * (E1 - E2) / eta\n",
    "        # Clip a2 based on bounds L & H\n",
    "        if L < a2 < H:\n",
    "            a2 = a2\n",
    "        elif (a2 <= L):\n",
    "            a2 = L\n",
    "        elif (a2 >= H):\n",
    "            a2 = H\n",
    "            \n",
    "    # If eta is non-negative, move new a2 to bound with greater objective function value\n",
    "    else:\n",
    "        alphas_adj = model.alphas.copy()\n",
    "        alphas_adj[i2] = L\n",
    "        # objective function output with a2 = L\n",
    "        Lobj = objective_function(alphas_adj, model.y, model.kernel, model.X) \n",
    "        alphas_adj[i2] = H\n",
    "        # objective function output with a2 = H\n",
    "        Hobj = objective_function(alphas_adj, model.y, model.kernel, model.X)\n",
    "        if Lobj > (Hobj + eps):\n",
    "            a2 = L\n",
    "        elif Lobj < (Hobj - eps):\n",
    "            a2 = H\n",
    "        else:\n",
    "            a2 = alph2\n",
    "            \n",
    "    # Push a2 to 0 or C if very close\n",
    "    if a2 < 1e-8:\n",
    "        a2 = 0.0\n",
    "    elif a2 > (model.C - 1e-8):\n",
    "        a2 = model.C\n",
    "    \n",
    "    # If examples can't be optimized within epsilon (eps), skip this pair\n",
    "    if (np.abs(a2 - alph2) < eps * (a2 + alph2 + eps)):\n",
    "        return 0, model\n",
    "    \n",
    "    # Calculate new alpha 1 (a1)\n",
    "    a1 = alph1 + s * (alph2 - a2)\n",
    "    \n",
    "    # Update threshold b to reflect newly calculated alphas\n",
    "    # Calculate both possible thresholds\n",
    "    b1 = E1 + y1 * (a1 - alph1) * k11 + y2 * (a2 - alph2) * k12 + model.b\n",
    "    b2 = E2 + y1 * (a1 - alph1) * k12 + y2 * (a2 - alph2) * k22 + model.b\n",
    "    \n",
    "    # Set new threshold based on if a1 or a2 is bound by L and/or H\n",
    "    if 0 < a1 and a1 < C:\n",
    "        b_new = b1\n",
    "    elif 0 < a2 and a2 < C:\n",
    "        b_new = b2\n",
    "    # Average thresholds if both are bound\n",
    "    else:\n",
    "        b_new = (b1 + b2) * 0.5\n",
    "\n",
    "    # Update model object with new alphas & threshold\n",
    "    model.alphas[i1] = a1\n",
    "    model.alphas[i2] = a2\n",
    "    \n",
    "    # Update error cache\n",
    "    # Error cache for optimized alphas is set to 0 if they're unbound\n",
    "    for index, alph in zip([i1, i2], [a1, a2]):\n",
    "        if 0.0 < alph < model.C:\n",
    "            model.errors[index] = 0.0\n",
    "    \n",
    "    # Set non-optimized errors based on equation 12.11 in Platt's book\n",
    "    non_opt = [n for n in range(model.m) if (n != i1 and n != i2)]\n",
    "    model.errors[non_opt] = model.errors[non_opt] + \\\n",
    "                            y1*(a1 - alph1)*model.kernel(model.X[i1], model.X[non_opt]) + \\\n",
    "                            y2*(a2 - alph2)*model.kernel(model.X[i2], model.X[non_opt]) + model.b - b_new\n",
    "    \n",
    "    # Update model threshold\n",
    "    model.b = b_new\n",
    "    \n",
    "    return 1, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def examine_example(i2, model):\n",
    "    print \"examine example\"\n",
    "    print i2\n",
    "    print model\n",
    "    y2 = model.y[i2]\n",
    "    alph2 = model.alphas[i2]\n",
    "    E2 = model.errors[i2]\n",
    "    print E2\n",
    "    print y2\n",
    "    r2 = np.array(y2) * E2\n",
    "    print r2\n",
    "    # Proceed if error is within specified tolerance (tol)\n",
    "    if ((r2 < -tol and alph2 < model.C) or (r2 > tol and alph2 > 0)):\n",
    "        \n",
    "        if len(model.alphas[(model.alphas != 0) & (model.alphas != model.C)]) > 1:\n",
    "            # Use 2nd choice heuristic is choose max difference in error\n",
    "            if model.errors[i2] > 0:\n",
    "                i1 = np.argmin(model.errors)\n",
    "            elif model.errors[i2] <= 0:\n",
    "                i1 = np.argmax(model.errors)\n",
    "            step_result, model = take_step(i1, i2, model)\n",
    "            if step_result:\n",
    "                return 1, model\n",
    "            \n",
    "        # Loop through non-zero and non-C alphas, starting at a random point\n",
    "        for i1 in np.roll(np.where((model.alphas != 0) & (model.alphas != model.C))[0],\n",
    "                          np.random.choice(np.arange(model.m))):\n",
    "            step_result, model = take_step(i1, i2, model)\n",
    "            if step_result:\n",
    "                return 1, model\n",
    "        \n",
    "        # loop through all alphas, starting at a random point\n",
    "        for i1 in np.roll(np.arange(model.m), np.random.choice(np.arange(model.m))):\n",
    "            step_result, model = take_step(i1, i2, model)\n",
    "            if step_result:\n",
    "                return 1, model\n",
    "    \n",
    "    return 0, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    print \"training model\"\n",
    "    numChanged = 0\n",
    "    examineAll = 1\n",
    "\n",
    "    while(numChanged > 0) or (examineAll):\n",
    "        numChanged = 0\n",
    "        if examineAll:\n",
    "            # loop over all training examples\n",
    "            for i in range(model.alphas.shape[0]):\n",
    "                examine_result, model = examine_example(i, model)\n",
    "                numChanged += examine_result\n",
    "                if examine_result:\n",
    "                    obj_result = objective_function(model.alphas, model.y, model.kernel, model.X)\n",
    "                    model._obj.append(obj_result)\n",
    "        else:\n",
    "            # loop over examples where alphas are not already at their limits\n",
    "            for i in np.where((model.alphas != 0) & (model.alphas != model.C))[0]:\n",
    "                examine_result, model = examine_example(i, model)\n",
    "                numChanged += examine_result\n",
    "                if examine_result:\n",
    "                    obj_result = objective_function(model.alphas, model.y, model.kernel, model.X)\n",
    "                    model._obj.append(obj_result)\n",
    "        if examineAll == 1:\n",
    "            examineAll = 0\n",
    "        elif numChanged == 0:\n",
    "            examineAll = 1\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model\n",
      "examine example\n",
      "0\n",
      "<__main__.SMOModel instance at 0x000000000B3DA408>\n",
      "[[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1.]]\n",
      "[[ 1.]]\n",
      "[[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1.]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-fdc3c1d737be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-167-904f753f3c98>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[1;31m# loop over all training examples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malphas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0mexamine_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexamine_example\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0mnumChanged\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mexamine_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mexamine_result\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-166-e4ff0ee3e2ad>\u001b[0m in \u001b[0;36mexamine_example\u001b[0;34m(i2, model)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mr2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[1;31m# Proceed if error is within specified tolerance (tol)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr2\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mtol\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0malph2\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mr2\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mtol\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0malph2\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malphas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malphas\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malphas\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "output = train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.optimize as opt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"C:/Users/Niku/Documents/dataset/arrays.xlsx\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Normdata = np.matrix(data)\n",
    "cols = Normdata.shape[1]\n",
    "X = Normdata[:,0:2]\n",
    "y = Normdata[:,cols-1:cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.matrix(X)\n",
    "y = np.matrix(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state=1)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train, y_train)\n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train[y_train == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set model parameters and initial values\n",
    "C = 1000.0\n",
    "m = len(X_train_scaled)\n",
    "initial_alphas = np.zeros((1,m))\n",
    "initial_b = 0.0\n",
    "#print initial_alphas.shape\n",
    "# Set tolerances\n",
    "tol = 0.01 # error tolerance\n",
    "eps = 0.01 # alpha tolerance\n",
    "\n",
    "# Instantiate model\n",
    "model = SMOModel(X_train_scaled, y_train, C, linear_kernel,\n",
    "                 initial_alphas, initial_b, np.zeros(m))\n",
    "print \"model instantiated\"\n",
    "# Initialize error cache\n",
    "initial_error = decision_function(model.alphas, model.y, model.kernel, model.X, model.X, model.b) - model.y\n",
    "model.errors = initial_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initial_alphas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "output = train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initial_error.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
